{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow-Helper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOqMAx0hfmDJg0aMRdDuswe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RamezNabil/TensorFlow-helper-functions-and-blocks/blob/main/TensorFlow_Helper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pzt8HavvHHEs"
      },
      "source": [
        "##Setting up Kaggle api"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D96cxy-eGnNV"
      },
      "source": [
        "!pip install -q kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vDDXdT8HQHn"
      },
      "source": [
        "# Upload your Kaggle API json file\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irbtHi5AkZ01"
      },
      "source": [
        "!rm -r ~/.kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!mv ./kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8x5hujBKHlPI"
      },
      "source": [
        "##Downloading, Unzipping, Saving, and Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McnpFXxHHrhP"
      },
      "source": [
        "!kaggle competitions download -c street-view-getting-started-with-julia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiRQYACLb6Kx"
      },
      "source": [
        "# Unzipping the dataset into content/Dataset\n",
        "!unzip \"/content/trainResized.zip\" -d Dataset\n",
        "!unzip \"/content/testResized.zip\" -d Dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vmfu9HzhiE3S"
      },
      "source": [
        "# Removing a file from a directory\n",
        "%rm -rf Dataset/IDC_regular_ps50_idx5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPQqj1-EkAsr"
      },
      "source": [
        "# Making csv file for submission\n",
        "df = pd.DataFrame({'ID':ids, 'Class':y_pred})\n",
        "df.to_csv(r'submission.csv', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWc_wH_Ukfg9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvCfYju7qNjq"
      },
      "source": [
        "## Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUKejXBsqP0F"
      },
      "source": [
        "# Shuffle training dataframe\n",
        "train_df_shuffled = train_df.sample(frac=1, random_state=42) # shuffle with random_state=42 for reproducibility"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pQqZnopi_rQ"
      },
      "source": [
        "## Sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L71eyy40jLYj"
      },
      "source": [
        "# Splitting data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qedw26ZBjR0A"
      },
      "source": [
        "# Transforming y to numbered labels\n",
        "from sklearn import preprocessing\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(sorted(data_labels['Class'].unique())) # takes sorted list of unique class labels\n",
        "y = le.transform(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fW7zi77VmrwS"
      },
      "source": [
        "# Naive Bayes with tfidf vectorizer (used as a baseline in nlp)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Create tokenization and modelling pipeline\n",
        "model_0 = Pipeline([\n",
        "                    (\"tfidf\", TfidfVectorizer()), # convert words to numbers using tfidf\n",
        "                    (\"clf\", MultinomialNB()) # model the text\n",
        "])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "model_0.fit(train_sentences, train_labels)\n",
        "\n",
        "# Evaluation\n",
        "baseline_score = model_0.score(val_sentences, val_labels)\n",
        "\n",
        "# Making predictions\n",
        "baseline_preds = model_0.predict(val_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLaUofRUk0g2"
      },
      "source": [
        "## Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9dAoV_Dk2kQ"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "text_vectorizer = TextVectorization(max_tokens=None, # how many words in the vocabulary (all of the different words in your text)\n",
        "                                    standardize=\"lower_and_strip_punctuation\", # how to process text\n",
        "                                    split=\"whitespace\", # how to split tokens\n",
        "                                    ngrams=None, # create groups of n-words?\n",
        "                                    output_mode=\"int\", # how to map tokens to numbers\n",
        "                                    output_sequence_length=None) # how long should the output sequence of tokens be?\n",
        "                                    # pad_to_max_tokens=True) # Not valid if using max_tokens=None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdioS9wZladS"
      },
      "source": [
        "# Fit the text vectorizer to the training text\n",
        "text_vectorizer.adapt(train_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeCeGOl8lh8u"
      },
      "source": [
        "# Get the unique words in the vocabulary\n",
        "words_in_vocab = text_vectorizer.get_vocabulary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byCUv6mBljFm"
      },
      "source": [
        "## Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaDq53jRl2VJ"
      },
      "source": [
        "* input_dim - The size of the vocabulary (e.g. len(text_vectorizer.get_vocabulary()).\n",
        "* output_dim - The size of the output embedding vector, for example, a value of 100 outputs a feature vector of size 100 for each word.\n",
        "* embeddings_initializer - How to initialize the embeddings matrix, default is \"uniform\" which randomly initalizes embedding matrix with uniform distribution. This can be changed for using pre-learned embeddings.\n",
        "* input_length - Length of sequences being passed to embedding layer.\n",
        "\n",
        "***Note: Embedding takes the output of the vectorizer as an input***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBwdwWRSllR1"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "embedding = layers.Embedding(input_dim=max_vocab_length, # set input shape\n",
        "                             output_dim=128, # set size of embedding vector\n",
        "                             embeddings_initializer=\"uniform\", # default, intialize randomly\n",
        "                             input_length=max_length) # how long is each input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKKq6pNvmf1T"
      },
      "source": [
        "# Embed the random sentence (turn it into numerical representation)\n",
        "sample_embed = embedding(text_vectorizer([random_sentence]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zw1bOdiTiaRo"
      },
      "source": [
        "## Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tCCFB56ib0c"
      },
      "source": [
        "# Unfreezing some layers for fine-tuning\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:-10]:\n",
        "  layer.trainable = False\n",
        "\n",
        "# Fit the model\n",
        "history_finetune = model.fit(x=X_train, y=y_train, validation_data=(X_test, y_test), epochs=10, initial_epoch = history.epoch[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOjFWUL3iqjL"
      },
      "source": [
        "# plotting val accuracy before and after fine-tuning with a seperating line\n",
        "val_acc = history.history[\"val_accuracy\"]\n",
        "total_val_acc = val_acc + history_finetune.history[\"val_accuracy\"]\n",
        "\n",
        "plt.plot(total_val_acc, label='Validation Accuracy')\n",
        "plt.plot([4, 4], plt.ylim(), label='Start Fine Tuning')\n",
        "plt.legend(loc='lower right')\n",
        "plt.xlabel('Epochs')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQgRak8vncoi"
      },
      "source": [
        "## Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHTv_nV6neaJ"
      },
      "source": [
        "def create_tensorboard_callback(dir_name, experiment_name):\n",
        "  \"\"\"\n",
        "  Creates a TensorBoard callback instance to store log files.\n",
        "  Stores log files with the filepath:\n",
        "    \"dir_name/experiment_name/current_datetime/\"\n",
        "  Args:\n",
        "    dir_name: target directory to store TensorBoard log files\n",
        "    experiment_name: name of experiment directory (e.g. efficientnet_model_1)\n",
        "  \"\"\"\n",
        "  log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "      log_dir=log_dir\n",
        "  )\n",
        "  print(f\"Saving TensorBoard log files to: {log_dir}\")\n",
        "  return tensorboard_callback"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-F2zKKPMnHPw"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffq58YVgnUIM"
      },
      "source": [
        "# Function to evaluate: accuracy, precision, recall, f1-score\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "def calculate_results(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Calculates model accuracy, precision, recall and f1 score of a binary classification model.\n",
        "\n",
        "  Args:\n",
        "  -----\n",
        "  y_true = true labels in the form of a 1D array\n",
        "  y_pred = predicted labels in the form of a 1D array\n",
        "\n",
        "  Returns a dictionary of accuracy, precision, recall, f1-score.\n",
        "  \"\"\"\n",
        "  # Calculate model accuracy\n",
        "  model_accuracy = accuracy_score(y_true, y_pred) * 100\n",
        "  # Calculate model precision, recall and f1 score using \"weighted\" average\n",
        "  model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
        "  model_results = {\"accuracy\": model_accuracy,\n",
        "                  \"precision\": model_precision,\n",
        "                  \"recall\": model_recall,\n",
        "                  \"f1\": model_f1}\n",
        "  return model_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2chfx_Ku3sM"
      },
      "source": [
        "# How long of a sentence covers 95% of the lengths?\n",
        "output_seq_len = int(np.percentile(sent_lens, 95)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}